version: '3.8'

services:
  # Ollama for in-house LLM
  ollama:
    image: ollama/ollama:latest
    container_name: mezopotamya-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        limits:
          memory: 8G
    command: serve
    
  # Pull and run model
  ollama-setup:
    image: ollama/ollama:latest
    depends_on:
      - ollama
    volumes:
      - ollama:/root/.ollama
    entrypoint: >
      sh -c "
        sleep 10 &&
        ollama pull llama2:7b-chat &&
        ollama pull mistral:7b-instruct
      "

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: mezopotamya-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped

  # Python FastAPI Backend
  backend:
    build:
      context: ./mezopotamya-backend
      dockerfile: Dockerfile
    container_name: mezopotamya-backend
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - DATABASE_PATH=/app/data/mezopotamya.db
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
      - CHUNK_SIZE=512
      - CHUNK_OVERLAP=50
    volumes:
      - ./data:/app/data
    depends_on:
      - ollama
      - ollama-setup
      - qdrant
    restart: unless-stopped

  # Next.js Frontend
  frontend:
    build:
      context: ./mezopotamya-frontend
      dockerfile: Dockerfile
    container_name: mezopotamya-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://backend:8000
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  ollama:
  data:
  qdrant_storage:
